# AAM-ConvLSTM
Spatio-temporal Consistency Network for 360 Video Saliency Prediction
# abstract
360$^\circ$ videos have been widely used with the development of virtual reality technology and triggered a demand to determine
the most visually attractive objects in them, aka 360$^\circ$ video saliency prediction (VSP). While the spatio-temporal consistency has been proven to be useful in traditional VSP, the research on utilizing it in the scenario of 360$^\circ$ video is still insufficient due to the severe distortions and the ambiguity of capture-worthiness or saliency. In this study, we proposed a novel spatio-temporal consistency network for 360$^\circ$ VSP. A dual-stream encoder-decoder architecture is adopted to process the forward and backward frame sequences of 360$^{\circ}$ videos simultaneously. Moreover, an axial attention memory module is designed in the encoder and embedded into the spherical ConvLSTM to memorize features with global-range spatial and temporal dependencies. Finally, motivated by the bias phenomenon in human viewing behavior, a temporal-convolutional Gaussian prior module is introduced to further improve the accuracy of the saliency prediction. Extensive experiments are conducted to evaluate our model and the state-of-the-art competitors, demonstrating that our model has achieved the best performance on the databases of PVS-HM and VR-Eyetracking.
